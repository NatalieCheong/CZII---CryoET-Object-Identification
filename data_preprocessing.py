# -*- coding: utf-8 -*-
"""data_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HvTbx00ghyQRKMG7smCJ64sYMsYrlb8u
"""

# Extract patches with overlap (stride)
        for z in range(half_size, depth - half_size, stride):
            for y in range(half_size, height - half_size, stride):
                for x in range(half_size, width - half_size, stride):
                    # Extract the patch
                    # Extract the patch
                    patch = tomo_data[
                        z - half_size:z + half_size,
                        y - half_size:y + half_size,
                        x - half_size:x + half_size
                    ]

                    # Skip if patch is invalid
                    if patch.shape != (patch_size, patch_size, patch_size):
                        continue

                    patches.append(patch)
                    coordinates.append((x, y, z))

        # Convert to numpy arrays
        patches = np.array(patches)

        # Store test data
        test_data[experiment] = {
            'patches': patches,
            'coordinates': coordinates,
            'shape': tomo_data.shape
        }

        # Save the test data
        os.makedirs(os.path.join(output_dir, 'test', experiment), exist_ok=True)
        np.save(os.path.join(output_dir, 'test', experiment, 'patches.npy'), patches)
        np.save(os.path.join(output_dir, 'test', experiment, 'coordinates.npy'), coordinates)
        np.save(os.path.join(output_dir, 'test', experiment, 'shape.npy'), tomo_data.shape)

        # Print statistics
        print(f"Processed {len(patches)} patches for experiment {experiment}")

        # Free memory
        del tomo_data, patches
        gc.collect()

    return test_data

def plot_random_patches(patches, labels, metadata_df, n_samples=5):
    """
    Plot random patches and their labels

    Parameters:
    patches (numpy.ndarray): Array of patches
    labels (numpy.ndarray): Array of labels
    metadata_df (pandas.DataFrame): DataFrame with metadata
    n_samples (int): Number of samples to plot
    """
    # Get random indices
    indices = np.random.choice(len(patches), size=n_samples, replace=False)

    # Plot each sample
    for i, idx in enumerate(indices):
        patch = patches[idx]
        label = labels[idx]

        # Get metadata for this patch
        if i < len(metadata_df):
            p_type = metadata_df.iloc[idx]['particle_type']
            p_weight = metadata_df.iloc[idx]['weight']
        else:
            p_type = "Unknown"
            p_weight = "Unknown"

        # Plot the middle slice of the patch and label
        middle_slice = patch.shape[0] // 2

        plt.figure(figsize=(12, 6))

        # Plot patch
        plt.subplot(1, 2, 1)
        plt.imshow(patch[middle_slice], cmap='gray')
        plt.title(f"Patch {idx}: {p_type} (Weight: {p_weight})")
        plt.axis('off')

        # Plot label
        plt.subplot(1, 2, 2)
        plt.imshow(label[middle_slice], cmap='hot')
        plt.title(f"Label {idx}")
        plt.axis('off')

        plt.tight_layout()
        plt.show()

def main():
    # Set paths
    base_dir = '/kaggle/input/czii-cryo-et-object-identification'
    train_dir = os.path.join(base_dir, 'train')
    test_dir = os.path.join(base_dir, 'test')
    output_dir = '/kaggle/working/preprocessed'

    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    print("Starting data preprocessing...")

    # Set parameters
    patch_size = 64  # Size of cubic patches
    split_ratio = 0.2  # Ratio for validation split

    # Process training data
    train_patches, train_labels, val_patches, val_labels, metadata_df = process_training_data(
        train_dir, output_dir, patch_size, split_ratio
    )

    # Process test data
    test_data = process_test_data(test_dir, output_dir, patch_size, stride=32)

    print("Data preprocessing complete.")

if __name__ == "__main__":
    main()